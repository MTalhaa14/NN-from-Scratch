# -*- coding: utf-8 -*-  F2020376027 Muhammad Talha Qadri
"""P1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/

libraries
"""

import pandas as pd
import numpy as np
from pandas import DataFrame
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
from sklearn import preprocessing
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings("ignore")

"""Dataset Importing via lib

"""

from sklearn.datasets import load_boston
boston = load_boston()
print(boston)

"""Diving into X & Y

---


"""

df_x = pd.DataFrame(boston.data,columns = boston.feature_names)
df_y = pd.DataFrame(boston.target)

input_set = df_x.to_numpy()
labels = df_y.to_numpy()

"""Data spliting

"""

X_train, X_test, Y_train, Y_test = train_test_split(input_set, labels,test_size=0.33, random_state=0)

scaler = preprocessing.StandardScaler()# standardising values
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Basic Functions and their dev"""

def sigmoid(x):#sigmoid 
    return 1/(1+np.exp(-x))

def sigmoid_derivative(x):#sig_dev
    return sigmoid(x)*(1-sigmoid(x))

def Relu(x):#relu
  if(x>0):
    return x
  else:
    return 0

def dRelu(x):#relu_dev
  if(x>0):
    return 1.
  else:
    return 0



def forward_pass(xtr,w,b):#forward pass using mathematical formula
  nety=np.dot(xtr,w)+b  
  outy=Relu(nety)
  return outy

def Backward_pass(xtr,out,w,b,label,lr): #back prop using mathematical formula and updating weights
  error=out-label
  for i in range(len(xtr)):
    deltaw=error*dRelu(out)*xtr[i] #delta rule
    w[i]=w[i]-lr*deltaw
  return w

def Stochastic(xtr):
  np.random.seed(5)                             #Random weights & bias
  w = np.random.uniform(0,1,(13,1))
  b = np.random.uniform(0,1,1)
  lr = 0.1                                    #learning rate

  for epoch in range(50):                     #epoch calculation using forward & back fun 
    l=[ ]
    y_pred = []
    for i in range(len(xtr)):
      xtrain=xtr[i]
      ytr=Y_train[i]
      y=forward_pass(xtrain,w,b)
      w=Backward_pass(xtrain,y,w,b,ytr,lr)
      y_pred.append(y)
    loss = mean_squared_error(y_pred,Y_train)
    print("Epoch:%d_____Loss:%.6f _____"%(epoch,loss))
  return w,b,lr,loss

def predict(xtest,w,b):               #prediction for Y
    y_pred=[]
    for i in range(len(xtest)):
        xtes=xtest[i]
        out=np.dot(xtes,w)+b
        y_pred.append(out)
    return np.array(y_pred)

w,b,a,c =Stochastic(X_train)
print(w,b)
Y_pred=predict(X_test,w,b)

"""plotting

"""

from matplotlib.pyplot import figure          #comparing Actual & predicted grphs 
plt.figure(figsize=(20,10))
plt.plot(Y_test, label='Actual')
plt.plot(Y_pred, label='Predicted')
plt.legend(prop={'size': 16})
plt.show()
print('Mean Squared Error :',mean_squared_error(Y_test, Y_pred))

def grph (loss1):
  plt.plot(loss1)
  plt.show()

w,b,lr,loss1 = Stochastic(X_train)
Y_pred=predict(X_test,w,b)
print(Y_pred)
grph(loss1)
